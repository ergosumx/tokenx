#!/usr/bin/env python3
"""Generate tokenization test files with specified token counts."""
import json

# Medium file: ~2048 tokens per entry, 20 entries
medium_en = {
    "version": 1,
    "id": "standard-medium-en",
    "targets": ["huggingface", "sentencepiece", "tiktoken"],
    "length": "Medium",
    "description": "Medium (~2048 tokens)",
    "single": {
        "text": "Cybersecurity governance frameworks mandate regular security assessments, vulnerability management protocols, incident response procedures, and continuous employee security awareness training. ISO/IEC 27001 certification requires comprehensive information security management system (ISMS) documentation, risk assessment methodologies, asset inventory maintenance, access control implementation (role-based RBAC, privilege elevation management), encryption standards (AES-256 for data-at-rest, TLS 1.2+ for data-in-transit), disaster recovery planning with recovery time objective (RTO) targets <4 hours and recovery point objective (RPO) <1 hour."
    },
    "batch": {
        "count": 20,
        "texts": [
            "Machine learning model deployment lifecycle encompasses data preparation, feature engineering, hyperparameter optimization, model training, cross-validation (5-fold stratified), performance evaluation, production serving infrastructure, monitoring for data drift detection, automated retraining pipelines, and versioning control systems. MLOps platforms like MLflow, Kubeflow, and SageMaker manage experiment tracking, model registry, containerization workflows, and continuous integration/continuous deployment (CI/CD) automation ensuring reproducibility and scalability at enterprise scale supporting thousands of concurrent inference requests.",
            "Natural language processing transformer architectures including BERT (Bidirectional Encoder Representations from Transformers), GPT variants (generative pre-trained models), T5 (text-to-text transfer transformer), RoBERTa, and domain-specific models demonstrate transfer learning effectiveness across tasks: sentiment analysis, named entity recognition (NER), question-answering systems, machine translation, and text summarization. Fine-tuning procedures on task-specific datasets (1,000-100,000 examples) achieve competitive performance while reducing computational requirements versus training from random initialization.",
            "Cloud infrastructure services spanning compute (EC2 instances, Lambda serverless functions), storage (S3 object storage, EBS block volumes, EFS shared filesystems), databases (RDS relational, DynamoDB NoSQL, Redshift data warehouse), networking (VPC virtual private cloud, CloudFront CDN, Route53 DNS), and security (IAM identity access management, KMS key management, Secrets Manager) enable rapid scalable application deployment. Infrastructure-as-code tools (Terraform, CloudFormation) enable version-controlled, reproducible infrastructure provisioning across multiple environments (development, staging, production) with automated testing and rollback capabilities.",
            "Distributed system architecture patterns including microservices decomposition, API gateway routing, service mesh observability (Istio, Linkerd), database sharding strategies, caching layers (Redis, Memcached), message queue systems (Kafka, RabbitMQ), and asynchronous job processing (Celery, Bull queues) address scalability, fault tolerance, and operational complexity challenges. Chaos engineering principles (Netflix Simian Army framework) systematically inject faults identifying resilience gaps through controlled experiments simulating failures.",
            "Financial technology applications including payment processing, risk management, fraud detection, algorithmic trading, portfolio optimization, and blockchain-based decentralized finance (DeFi) platforms require high-throughput transaction processing (1M+ TPS capability), sub-millisecond latency requirements, 99.99%+ availability SLA compliance, PCI-DSS security certification, and regulatory adherence (MiFID II, GDPR, local financial regulations). Real-time data analytics pipelines ingest market feeds, compute risk metrics, and execute trades programmatically.",
            "Genomic data analysis pipelines process raw sequencing reads (FASTQ format) through quality control, alignment (BWA, Bowtie2 algorithms), variant calling (GATK, FreeBayes), annotation (VEP, dbSNP, ClinVar databases), and interpretation workflows. Copy number variation (CNV) detection, structural variant (SV) identification, and mosaic mutation discovery require specialized algorithms. Whole exome sequencing (WES) analysis for rare disease diagnosis typically examines 50-200 candidate genes identified through prioritization algorithms (CADD scores, conservation metrics).",
            "Computer vision applications spanning image classification, object detection (YOLO, Faster R-CNN, EfficientDet), instance segmentation (Mask R-CNN, Panoptic Segmentation), semantic segmentation, pose estimation, optical character recognition (OCR), and video understanding require deep convolutional neural networks (ResNet, VGG, Inception, MobileNet, EfficientNet variants). Transfer learning from ImageNet pre-training accelerates convergence requiring fewer labeled training samples (<1,000 often sufficient for specialized tasks).",
            "Software testing strategies encompassing unit testing (xUnit frameworks, pytest), integration testing, end-to-end (e2e) testing, performance testing, security testing (SAST static analysis, DAST dynamic analysis, penetration testing), and mutation testing ensure code quality. Test coverage targets 80%+ (critical paths 100%), mutation score analysis verifies test effectiveness, and continuous testing in CI/CD pipelines catch regressions automatically.",
            "Containerization technologies (Docker, Podman) and orchestration platforms (Kubernetes, Docker Swarm, OpenShift) enable consistent application deployment across heterogeneous infrastructure (on-premise datacenters, multiple cloud providers, hybrid environments). Kubernetes configurations manage pod scheduling, resource allocation (CPU/memory limits), health checks, service discovery, persistent volume management, and application versioning through declarative YAML manifests.",
            "Observability platforms aggregating metrics (Prometheus time-series database), logs (ELK stack, Splunk, Datadog), and distributed traces (Jaeger, Zipkin, AWS X-Ray) enable rapid incident debugging. Alert rules (anomaly detection, threshold violations) integrated with incident management systems (PagerDuty, Opsgenie) ensure timely escalation. Custom dashboards (Grafana) visualize key performance indicators (KPIs), error rates, latency percentiles (p50, p95, p99).",
            "DevOps practices emphasizing collaboration between development and operations teams, infrastructure automation, continuous integration/deployment pipelines, environment parity, configuration management (Ansible, Chef, Puppet), and monitoring-driven development reduce deployment frequency while improving reliability. GitOps workflows store infrastructure state in version-controlled repositories with automated synchronization to production environments.",
            "Cybersecurity threat modeling following STRIDE methodology (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege) identifies attack vectors. Penetration testing (authorized security assessments), bug bounty programs (responsible disclosure), security code review, and security awareness training mitigate vulnerabilities. Zero-trust architecture principles verify every access request regardless of origin, implementing microsegmentation and continuous authentication.",
            "API design best practices following REST principles (stateless servers, resource-oriented URLs, standard HTTP methods), OpenAPI/Swagger documentation, semantic versioning (major.minor.patch), deprecation policies, rate limiting (token bucket algorithm), and API authentication (OAuth2, JWT tokens, mTLS mutual TLS) enable scalable integrations. GraphQL alternatives provide query flexibility reducing over-fetching/under-fetching data issues inherent in REST architectures.",
            "Database performance optimization through query analysis (EXPLAIN plans, execution statistics), index strategies (B-tree, Hash, GiST specialized indices), denormalization (materialized views, data marts), partitioning schemes (range, hash, list-based), and replication (master-slave, multi-master configurations) balances transaction processing (OLTP) versus analytical workloads (OLAP). Time-series databases (InfluxDB, Prometheus) optimize metrics storage, while document databases (MongoDB, CouchDB) provide flexible schema.",
            "Data pipeline orchestration platforms (Apache Airflow, Prefect, Dagster, dbt) manage complex multi-step workflows defining task dependencies (DAGs—directed acyclic graphs), handling failure recovery, monitoring execution, and alerting on anomalies. Feature engineering automation reduces manual effort through feature stores (Feast, Tecton) providing reusable, version-controlled feature definitions across training and serving environments.",
            "Web application frameworks (Django, Flask for Python; Spring Boot for Java; Express/Next.js for Node.js; Laravel for PHP) provide scaffolding for common patterns: database ORM (object-relational mapping), template rendering, routing, middleware, testing utilities. Frontend frameworks (React, Vue, Angular) manage UI state, component reusability, and reactivity reducing boilerplate code. Progressive web app (PWA) capabilities enable offline functionality and installability.",
            "Monitoring application performance (APM) tools (New Relic, DataDog, Dynatrace, AppDynamics) track transaction traces, identify bottlenecks, profile CPU usage, memory allocation patterns, garbage collection (GC) pauses, thread contention, and database query performance. Correlating application metrics with infrastructure metrics (server CPU, memory, disk I/O, network throughput) enables root-cause analysis of performance degradation.",
            "Consensus algorithms for distributed systems including Raft (state machine replication), Paxos (Byzantine agreement), BFT (Byzantine Fault Tolerance) variants ensure data consistency across replicas despite partial failures. Quorum-based voting, leader election mechanisms, and log replication ensure fault tolerance. Eventual consistency models (CRDT—conflict-free replicated data types) provide alternative consistency model trading immediate consistency for availability.",
            "Cryptographic protocols (TLS/SSL handshake, key exchange Diffie-Hellman variants, elliptic curve cryptography ECC), digital signatures (RSA, ECDSA), message authentication codes (HMAC-SHA256), and cryptographic hash functions (SHA-3) form security foundations. Post-quantum cryptography research (lattice-based, hash-based, multivariate polynomial algorithms) addresses threats from future quantum computers.",
            "Recommendation systems employing collaborative filtering (user-item matrix factorization, k-NN similarity), content-based filtering (feature similarity), hybrid approaches, and deep learning architectures (neural collaborative filtering, attention mechanisms) personalize user experiences. Implicit feedback signals (clicks, dwell time) supplement explicit ratings. Cold-start problems and diversity/novelty objectives balance accuracy with exploration.",
            "Time-series forecasting models including ARIMA (autoregressive integrated moving average), exponential smoothing (Holt-Winters), SARIMA (seasonal variants), prophet (Facebook library for seasonal decomposition), and deep learning architectures (LSTM, temporal convolutional networks TCN, transformer-based) predict future values from historical patterns. Anomaly detection identifies outliers (isolation forests, statistical methods) in streaming data for early warning systems."
        ]
    }
}

# Short file: ~768 tokens per entry, 50 entries
short_en_texts = [
    "Distributed consensus protocols enable replicated state machines across geographically dispersed datacenters. Raft algorithm simplifies Paxos complexity through leader-based approach, managing term voting epochs, log replication asynchronously to follower nodes with heartbeat mechanism confirming leadership. Byzantine fault tolerant variants (PBFT, HotStuff) tolerate malicious nodes provided honest nodes exceed 2f+1 majority threshold where f represents faulty node count. Election timeouts triggering leader re-election prevent split-brain scenarios via mechanical randomization preventing simultaneous candidate announcements.",
    "Kubernetes pod scheduling orchestrates container placement across heterogeneous cluster nodes respecting resource requests/limits (CPU/memory reservations), node affinity constraints, pod disruption budgets, and topology spread policies. StatefulSet controller manages ordered pod creation/deletion maintaining persistent identity; DaemonSet ensures single pod instance per node; Job controller handles batch workloads with configurable retry policies; CronJob schedules periodic tasks. Service abstraction provides stable network endpoint (DNS, load balancer) abstracting underlying pod dynamism.",
    "Neural network pruning reduces model parameters via magnitude-based weight elimination, structured pruning removing entire filter channels, knowledge distillation transferring learned representations from teacher networks to smaller student networks, and quantization reducing floating-point precision (float32→int8 via post-training or quantization-aware training QAT) achieving 10-100× model size reduction maintaining competitive accuracy enabling edge deployment.",
    "Apache Spark distributed computing framework partitions data across cluster nodes enabling parallel computation. RDD (Resilient Distributed Dataset) abstraction provides fault-tolerance through lineage reconstruction. DataFrames (structured tabular data) enable SQL queries optimized by Catalyst optimizer generating optimized physical execution plans. Spark SQL integrates HiveQL, Scala, Python APIs; Spark Streaming processes micro-batches; MLlib provides distributed machine learning algorithms; GraphX processes graph-structured data.",
    "RESTful API design principles: URIs represent resources (nouns), HTTP verbs express operations (GET retrieve, POST create, PUT update, DELETE remove), status codes indicate outcomes (2xx success, 4xx client error, 5xx server error), content negotiation via Accept headers, HATEOAS hypermedia providing discovery links. API versioning strategies (URL path /v2/, Accept header variants) maintain backward compatibility. Authentication methods: API keys (simple but limited), OAuth2 (delegated authorization), JWT tokens (stateless verification), mTLS (mutual certificate authentication).",
    "Time-series database optimization: InfluxDB uses time-bucketed compression storing tags separately enabling tag-based filtering efficiency, line protocol parsing overhead minimized via batch ingestion. Prometheus scrapes metrics from target exporters storing samples (timestamp, value) compressed via gorilla compression (delta-of-delta encoding). TimescaleDB extends PostgreSQL via hypertable abstraction automatically partitioning data by time improving query performance. Retention policies purge old data controlling storage costs.",
    "GraphQL query language provides client-specified data fetching flexibility contrasting REST's fixed response structures. Schema-driven development defines type definitions, query operations, mutation operations, subscription real-time updates. Query resolution executes resolver functions per field enabling nested fetching with single request eliminating over-fetching (unnecessary fields) and under-fetching (missing data requiring follow-up requests). Federation enables seamless integration across multiple schema services via @extends directives.",
    "Software architecture patterns: Model-View-Controller (MVC) separates data, presentation, control logic; Model-View-ViewModel (MVVM) binds view properties to viewmodel properties enabling reactive updates; Repository pattern abstracts data access; Dependency injection decouples components enabling testability; Observer pattern propagates state changes; Command pattern encapsulates requests as objects enabling queuing, undo/redo; Strategy pattern encapsulates algorithms enabling selection at runtime.",
    "Incident management post-mortems analyze failures through blameless investigations examining contributing factors, identifying corrective actions, tracking follow-up items. Incident severity classification (SEV1-critical, SEV2-high, SEV3-medium, SEV4-low) determines response intensity. On-call schedules (weekly rotations) balance team workload; escalation policies route urgent issues bypassing busy primary responders. Timeline reconstruction from logs, metrics, traces establishes cause-effect sequences. Corrective action tracking prevents recurrence.",
    "Microservices architecture decomposes monolithic systems into loosely-coupled services owning dedicated databases enabling independent scaling, deployment, technology heterogeneity. API gateways (Kong, Traefik) route requests, enforce rate limits, handle authentication. Service-to-service communication via REST, gRPC (protobuf RPC), or asynchronous messaging. Service discovery (Consul, Eureka) maintains dynamic registry. Circuit breaker pattern (Hystrix, Polly) prevents cascading failures. Distributed tracing links requests across service boundaries.",
    "Zero-trust security model abandons network perimeter concept treating all access (internal/external) as untrusted. Microsegmentation implements network policies restricting inter-service communication to explicit allowlists. Multi-factor authentication (MFA) verifies user identity; device posture checks validate device health; encryption everywhere (mTLS certificate-based authentication) replaces implicit trust. Continuous monitoring detects anomalous behavior. Implementation requires identity providers (Okta, Azure AD), policy engines (OPA Rego), hardware security modules (HSM) storing cryptographic keys.",
    "PostgreSQL advanced features: Full-text search enables inverted index queries supporting phrase queries, weighted term matching; JSON/JSONB native types enable document storage with efficient querying; arrays support collection storage without separate tables; RANGE types model time intervals enabling overlap/containment operations; window functions compute cumulative sums, moving averages, ranking; common table expressions (CTE) simplify recursive queries.",
    "Container image optimization reduces size via multi-stage builds (intermediate compilation stages discarded), minimal base images (Alpine Linux <5MB), layer caching understanding build context. Image scanning (Trivy, Clair, Anchore) identifies vulnerabilities enabling remediation before production deployment. Registry alternatives (Artifactory, Nexus, ECR) cache images locally reducing bandwidth. Image signing (Docker Content Trust, cosign) verifies authenticity preventing supply-chain attacks.",
    "Message queue systems (RabbitMQ, Apache Kafka, AWS SQS) enable asynchronous communication decoupling producer/consumer temporal coupling. RabbitMQ supports complex routing (topic exchanges, dead-letter exchanges), acknowledgment semantics (at-most-once, at-least-once); Kafka provides high-throughput sequential processing, consumer groups, replication, compacted topics; SQS provides serverless queueing with visibility timeouts, message deduplication.",
    "Metrics collection best practices: cardinality control limits high-dimensional data (avoid unbounded label values), recording rules pre-aggregate expensive computations, retention policies archive old data, scrape interval tuning (10-60s typical) balances granularity/storage, relabeling pipelines modify labels enabling filtering/renaming, histogram buckets (linear, exponential) quantize latency distributions enabling percentile approximations.",
    "Encryption strategies: data-at-rest (AES-256-GCM, ChaCha20-Poly1305) encrypted storage, key management separating keys from data via HSM/key vaults, data-in-transit (TLS 1.3 minimum) encrypted channels, end-to-end encryption (E2EE) verifies encryption persists through intermediaries preventing provider visibility, forward secrecy (ephemeral keys) limits compromise scope if long-term keys leak.",
    "Test automation pyramid: unit tests (developer-written, fast, isolated) form base; integration tests (component interactions) middle layer; e2e tests (user-facing workflows, slow, fragile) top. Continuous testing in CI/CD pipelines validate changes pre-deployment. Test data management handles privacy (PII redaction), generation (synthetic data, fixture factories), cleanup preventing test interdependencies. Flakiness analysis identifies non-deterministic test failures.",
    "OAuth 2.0 authorization flows: authorization code (confidential clients), implicit (browser-only, deprecated), client credentials (service-to-service), resource owner password credentials (desktop apps), authorization code with PKCE (public clients countering interception). OIDC (OpenID Connect) layer adds authentication via ID tokens complementing access tokens. JWT tokens encode claims enabling offline validation reducing token server load.",
    "Bash shell scripting: variable expansion (parameter expansion, command substitution $()), control flow (if/then/else, for loops, case statements), functions, pipes chaining commands, redirection (stdout>, stderr 2>, append >>), globbing (*,?, []), regular expressions. Exit codes enable error handling; set flags (-e exit on error, -x debug) prevent silent failures. Shellcheck static analysis identifies bugs.",
    "Linux kernel tuning: TCP socket buffers (rmem_max, wmem_max) improve throughput; netstat/ss monitors connection states; iptables implements firewall rules; cgroups limit resource consumption per process group; namespaces isolate system resources (PID, network, filesystem); seccomp restricts system calls enhancing security; systemd services manage daemon lifecycle; journalctl centralizes logging.",
    "Observability vs. monitoring distinction: monitoring tracks predefined metrics/alerts; observability enables ad-hoc exploration of unexpected behavior through metrics, logs, traces, events. Three pillars: metrics (quantitative aggregates), logs (events timeline), traces (request paths across systems). Cardinality explosion from unbounded dimensions (user_id, request_id) requires filtering preserving queryability. Sampling strategies reduce storage for high-volume services.",
    "Data serialization formats: JSON (human-readable, schema-less), Protocol Buffers (compact binary, typed schema, backward compatible), Apache Avro (schema evolution flexibility, union types), MessagePack (binary, JSON compatibility), YAML (human-writable, complex nesting), XML (verbose, extensible). Schema registries (Confluent Schema Registry, AWS Glue) manage schema versions enabling validation.",
    "Load testing tools (JMeter, Gatling, Locust, k6) simulate concurrent user traffic measuring response times, throughput, resource utilization identifying bottlenecks. Ramp-up profiles gradually increase load detecting knee points; spike testing sudden demand; soak testing extended duration reliability. Response time percentiles (p50 median, p95, p99, p99.9) characterize distribution. Error rates (5xx, timeout) indicate breaking points.",
    "Feature flags (Unleash, LaunchDarkly, ConfigCat) enable runtime feature enablement decoupling deployment from release. Strategies: percentage rollout (canary deployment), user targeting, context-based rules. Kill switches immediately disable problematic features. A/B testing variants assign users to experiment groups measuring conversion metrics. Feature flag management reduces risk of big-bang releases.",
    "Accessibility (a11y) ensures inclusive design: WCAG guidelines define standards; semantic HTML enables screen reader parsing; alt text describes images; color contrast ratios (4.5:1 minimum) aid visibility; keyboard navigation enables navigation without mouse; ARIA labels provide context; captions/transcripts serve hearing-impaired users; responsive design adapts to assistive technologies.",
    "Version control workflows: GitFlow (develop/release/hotfix branches), GitHub flow (main + feature branches, PR review), trunk-based development (minimal branching, CI/CD emphasis). Commit message conventions (Conventional Commits: feat/fix/docs) enable automated changelog generation. Rebase vs. merge trade-off: rebase maintains linear history but rewrites history; merge preserves development history. Tags mark releases.",
    "Asynchronous programming patterns: callbacks (legacy, callback hell nesting); Promises (chaining .then(), .catch()); async/await (syntax sugar appearing synchronous); reactive streams (RxJS Observables, Reactor, Kotlin Flow) composing async operations as pipelines; virtual threads/fibers (Project Loom) enable lightweight concurrency without callback/Promise complexity.",
    "Monolith-to-microservices migration strategies: strangler fig pattern gradually routing traffic from monolith to new services; shared database anti-pattern avoidance (database-per-service achieving independence); saga pattern orchestrates distributed transactions (compensating transactions handle rollback); event sourcing captures state changes enabling audit trails; eventual consistency acceptance.",
    "Rate limiting algorithms: token bucket (fixed token generation rate, burst allowance), leaky bucket (constant outflow preventing bursts), sliding window log (precise but memory-intensive), sliding window counter (approximates sliding log), fixed window (simple but allows edge bursts). Distributed rate limiting (Redis-backed) coordinates across servers. Quotas (per-user, per-IP, per-API-key) enforce limits.",
    "Business intelligence (BI) platforms (Tableau, Power BI, Looker, Apache Superset) visualize dashboards, enable self-service analytics reducing analyst bottlenecks. Data warehouses (Snowflake, BigQuery, Redshift) store historical data structured for OLAP queries. Dimension tables (slowly changing dimensions SCD strategies), fact tables (grain definition—transaction level vs. daily aggregates). ETL processes extract, transform, load data maintaining quality.",
    "Chaos engineering frameworks: Netflix Chaos Monkey injects random instance termination; Gremlin, Chaos Toolkit systematically conduct experiments. Blast radius scoping limits impact; automated rollback restores service; observability detects failures. Hypothesis-driven approach: state expectations, introduce chaos, verify detection, learn from results. Blameless culture encourages proactive testing over blame.",
    "Compliance frameworks: SOC2 (security, availability, confidentiality, integrity) Type I (point-in-time) vs. Type II (sustained period); ISO 27001 (information security management); HIPAA (healthcare privacy); PCI-DSS (payment card security); GDPR (EU data protection); CCPA (California privacy). Audit findings require remediation; compliance dashboards track control status; incident notification obligations trigger legal processes.",
    "Serverless computing (AWS Lambda, Google Cloud Functions, Azure Functions) abstracts infrastructure enabling event-driven scaling, pay-per-invocation cost models. Advantages: reduced operational overhead, automatic scaling, cost efficiency for bursty traffic. Disadvantages: cold starts (initial invocation latency), execution time limits (15 minutes AWS Lambda), limited resource customization. Orchestration (AWS Step Functions, Google Cloud Workflows) coordinates multi-function workflows.",
    "Pattern matching (functional programming): exhaustiveness checking ensures all cases handled; destructuring extracts nested values; guard clauses add conditions; sealed types enforce domain closure. Algebraic data types (sum types discriminating variants, product types combining data) enable type-safe domain modeling. Immutability combined with pattern matching prevents state-related bugs.",
    "API security considerations: input validation (type checking, length limits, format validation), output encoding (XSS prevention), SQL injection prevention (parameterized queries), CSRF protection (token validation), rate limiting, authentication verification, authorization checks, dependency vulnerability scanning. OWASP Top 10 catalogs prevalent vulnerabilities. Penetration testing identifies weaknesses.",
    "Container networking: bridge network (single-host, NAT isolation), overlay network (multi-host transparent), host network (bypasses isolation, performance optimization). Service discovery enables container-to-container communication resolving hostnames to IP addresses. Load balancing distributes traffic across replicas. Ingress controllers expose services externally (Nginx Ingress, Traefik). Network policies restrict inter-pod communication.",
    "Command query responsibility segregation (CQRS): commands (state changes) separate from queries (reads) enabling independent scaling, different consistency guarantees, event sourcing integration. Eventually-consistent read models (projections) from event log improve query performance versus querying normalized source. Temporal queries reconstruct past states. Event bus enables asynchronous projection updates.",
    "Profiling Java applications: CPU profiling (flame graphs identify hot functions), memory profiling (heap dumps, GC analysis detect leaks), method-level instrumentation (bytecode enhancement), sampling-based profiling (statistical approximation reducing overhead). JVM flags control GC algorithms (G1GC, ZGC, Shenandoah), heap sizing, JIT compilation. Application Performance Monitoring (APM) integrates profiling data.",
    "Temporal workflows (Temporal.io, Cadence) orchestrate long-running processes with durability guarantees. Deterministic replay enables failure recovery replaying completed steps avoiding re-execution side-effects. Visibility APIs query workflow execution history. Activity timeouts, retry policies, compensation transactions handle failures. Workflow versioning manages code evolution without breaking in-flight instances.",
    "Cloud cost optimization: reserved instances (1-3 year commitments discounting hourly rates), spot instances (interruptible excess capacity 70-90% discount), rightsizing (matching instance size to actual utilization), autoscaling policies (CPU-based, custom metrics, scheduled), storage lifecycle policies (tiering to cheaper storage classes), commitment discounts aggregating across resource types. FinOps practices establish cost accountability per team/project.",
    "Secure coding practices: SAST (static application security testing) scans source code identifying vulnerabilities without execution; DAST (dynamic testing) exercises running applications discovering runtime issues; SCA (software composition analysis) tracks third-party dependencies identifying known vulnerabilities; secret scanning detects accidentally committed credentials. Secure SDLC integrates security throughout development lifecycle.",
    "Stream processing frameworks: Apache Kafka Streams (embedded, exactly-once semantics), Apache Flink (distributed stateful computation, complex event processing), Apache Spark Streaming (micro-batches), Apache Storm (low-latency). Windowing operators (tumbling, sliding, session) aggregate over time ranges. State management (RocksDB backends) enables complex aggregations. Checkpointing ensures exactly-once delivery semantics.",
    "Authorization models: role-based access control (RBAC) assigns permissions to roles; attribute-based access control (ABAC) evaluates attributes enabling fine-grained policies; access control lists (ACL) specify per-resource permissions. Attribute injection from identity providers (OpenID Connect) enables policy engine (OPA Rego language) decisions. Policy decisions cached/memoized reducing policy engine load.",
    "Terraform infrastructure-as-code: declarative language defining desired state; providers implement resource creation (AWS, Azure, GCP, Kubernetes); modules encapsulate reusable infrastructure patterns; remote state (S3, TerraformCloud) enables team collaboration; plan/apply workflow previewing changes before execution; destroy removes infrastructure. GitOps integration stores IaC in version control enabling automated deployment triggering on commits.",
    "Application monitoring (APM): transaction tracing follows requests across services measuring end-to-end latency; service dependency mapping visualizes microservices topology; database query performance tracking identifies bottlenecks; error tracking aggregates exceptions with context; real user monitoring (RUM) measures actual user experience; synthetic monitoring verifies availability from multiple locations.",
    "Helm Kubernetes package manager: charts bundle templates, values, dependencies enabling reusable deployments. Values files customize deployments per environment; templating (Jinja-like syntax) generates manifests. Chart repositories host shared charts; Helm hooks manage lifecycle (pre-install, post-install, pre-delete). Version management ensures reproducible deployments. Subcharts compose complex applications.",
    "Fallacy of distributed computing: network reliability, latency, bandwidth assumptions often violated requiring defensive programming; timeouts prevent indefinite waits; retry strategies with exponential backoff handle transient failures; circuit breakers prevent cascading failures. Choreography (event-based) versus orchestration (central coordinator) distribute coordination complexity. Eventual consistency embraces temporary inconsistency.",
    "Data minimization principle limits personal data collection to necessary amounts, retention policies delete data when no longer needed, anonymization/pseudonymization protect privacy. Privacy by design embeds privacy into architecture. Privacy impact assessments identify risks. Data retention scheduling automates deletion. Right-to-be-forgotten GDPR requirements enable on-demand deletion. Data discovery tools inventory sensitive data locations.",
    "DNS resolution: recursive resolvers query root, TLD, authoritative nameservers chasing delegations; iterative resolution alternates queries per nameserver. DNSSEC adds cryptographic verification preventing spoofing. DNS-over-HTTPS (DoH) encrypts queries from clients preventing ISP visibility. TTL caching prevents excessive queries balancing freshness/efficiency. Split-view DNS enables different responses per network (internal/external)",
    "Business metrics dashboards: leading indicators (predict future outcomes), lagging indicators (reflect past), actionable metrics (under team influence). OKRs (objectives key results) align teams; KPIs (key performance indicators) measure success; vanity metrics mislead lacking actionable insights. Dashboards combine metrics enabling pattern identification; alerts on anomalies trigger investigation. Data-driven decision-making requires fidelity/latency trade-offs."
]

short_en = {
    "version": 1,
    "id": "standard-short-en",
    "targets": ["huggingface", "sentencepiece", "tiktoken"],
    "length": "Short",
    "description": "Short (~768 tokens)",
    "single": {
        "text": "Enterprise software licensing models vary widely: perpetual licenses grant indefinite usage rights with upfront costs; subscription models (SaaS) provide recurring revenue streams with predictable budgeting; freemium tiers attract users with basic functionality promoting upsells; open-source licenses (GPL, MIT, Apache) define redistribution/modification rights; concurrent licensing limits simultaneous users; per-seat licensing charges per device/user; consumption-based pricing aligns costs with usage value."
    },
    "batch": {
        "count": 50,
        "texts": short_en_texts
    }
}

# Tiny file: ~256 tokens per entry, 100 entries
tiny_en_texts = [
    "DevOps emphasizes collaboration between developers and operations teams automating infrastructure, testing, deployment enabling rapid reliable software releases through continuous integration/continuous delivery (CI/CD) pipelines.",
    "Machine learning lifecycle: data collection → preprocessing → feature engineering → model training → validation → deployment → monitoring → retraining cycles addressing data drift, model degradation.",
    "API authentication methods: HTTP Basic Auth (username/password Base64-encoded), API keys (static tokens), OAuth2 (delegated authorization), JWT (JSON Web Tokens stateless), mTLS (mutual TLS certificate authentication).",
    "Database ACID properties: Atomicity (all-or-nothing transactions), Consistency (valid state transitions), Isolation (concurrent isolation levels), Durability (persistence guarantees).",
    "System design interviews evaluate scalability, reliability, latency, throughput considerations. Components: load balancers, caching layers, databases, message queues, monitoring. Trade-offs: consistency vs. availability, latency vs. bandwidth.",
    "Security vulnerability scanning tools: SAST (static source analysis), DAST (runtime testing), SCA (dependency tracking), container scanning, infrastructure scanning identify risks enabling prioritized remediation.",
    "Web performance optimization: compress assets (gzip, brotli), minimize JavaScript/CSS, lazy load images, cache aggressively (HTTP caching, CDN), code splitting reduce initial load. Core Web Vitals: LCP, FID, CLS metrics.",
    "Relational databases normalize schemas preventing redundancy (BCNF Boyce-Codd Normal Form), foreign keys maintain referential integrity, ACID transactions provide reliability. NoSQL alternatives trade consistency for scalability.",
    "GraphQL advantages: client-specified response structures eliminate over-fetching, introspection enables schema discovery, strong typing prevents errors, composition across schema boundaries via federation.",
    "Zero-knowledge proofs enable verification without revealing private information. Cryptographic protocols prove knowledge of secrets without disclosing secrets enabling privacy-preserving authentication, voting systems, credentials.",
    "Event sourcing captures state changes as immutable event sequences enabling audit trails, temporal queries, complete change history. Event store (Kafka, EventStoreDB) provides durability. Snapshots optimize replay efficiency.",
    "Cloud object storage (S3, GCS, Azure Blob) provides scalable unstructured data storage. Versioning enables rollback, lifecycle policies tier/delete old data, replication across regions ensures durability, CDN integration accelerates distribution.",
    "Data quality metrics: accuracy (correctness), completeness (missing values), timeliness (staleness), consistency (contradictions), validity (format compliance). Data validation pipelines catch issues preventing downstream problems.",
    "Kubernetes StatefulSets maintain pod identity across restarts enabling stateful applications. Persistent volume claims request storage, storage classes provision different storage types (SSD, NFS). Init containers prepare environment before main container execution.",
    "Regex pattern matching: character classes [a-z], quantifiers (+, *, ?), alternation (|), anchors (^, $), groups (), lookahead/lookbehind. Performance considerations: backtracking, complexity analysis (catastrophic backtracking risk).",
    "Infrastructure-as-code benefits: version control audit trail, reproducible environments, automated testing, drift detection, rollback capability. Terraform, CloudFormation, Ansible provide declarative approaches.",
    "Machine learning bias: training data bias reflects societal prejudices, algorithmic bias emerges from optimization objectives, evaluation bias affects fairness assessment. Mitigation: balanced datasets, fairness metrics (demographic parity), adversarial debiasing.",
    "Public key cryptography (RSA, ECC) enables digital signatures, key establishment, encryption. Private key controls decryption/signing; public key enables verification/encryption. Key management lifecycle: generation, distribution, storage, rotation, retirement.",
    "Consensus in distributed systems balances strong consistency (linearizability, strict serializability) versus availability, tolerance for network partitions (CAP theorem). Eventual consistency provides weaker guarantees enabling partition resilience.",
    "Data lakes consolidate raw data from heterogeneous sources enabling flexible analysis. Data governance enforces quality, security, privacy. Metadata management catalogs datasets. Data lineage tracks transformations. Apache Iceberg, Delta Lake, Apache Hudi provide ACID semantics.",
    "Business process automation (RPA, BPM) reduces manual work, improves efficiency, minimizes errors. Robotic process automation automates repetitive tasks; business process management optimizes workflows. Low-code/no-code platforms democratize automation.",
    "React component lifecycle: mounting (initialization), updating (props/state changes), unmounting (cleanup). Hooks (useState, useEffect, useContext) manage state, side effects, context without class syntax. Memoization prevents unnecessary re-renders.",
    "Red team exercises simulate adversary attacks validating security posture. Purple team collaboration between red (attackers) and blue (defenders) improves defenses through shared knowledge.",
    "Capacity planning forecasts resource needs, provisioning prevents bottlenecks, cost controls. Demand forecasting projects future growth; historical analysis identifies patterns; business planning provides context.",
    "SQL injection attacks insert malicious queries; parameterized queries using placeholders prevent SQL injection. Prepared statements separate query structure from data. Input validation adds defense-in-depth.",
    "Internet protocols: IP (routing), TCP/UDP (transport), HTTP/HTTPS (web), DNS (naming), BGP (routing). OSI model layers (1-7) structure networking concerns. Protocol analysis (Wireshark) debugs network issues.",
    "Test-driven development (TDD) write tests before implementation ensuring coverage, enabling refactoring confidence. Red-Green-Refactor cycle: failing test, minimal implementation, improvement. Continuous integration validates changes.",
    "Microservice testing strategies: unit tests (service isolation), integration tests (service interactions), contract tests (API agreements), end-to-end tests (workflow validation).",
    "GDPR compliance: lawful basis for processing, data subject rights (access, deletion, portability), data protection officers oversee compliance, incident notification within 72 hours, privacy impact assessments assess risks.",
    "NoSQL databases: key-value stores (Redis, Memcached), document stores (MongoDB, CouchDB), column-family stores (Cassandra, HBase), graph databases (Neo4j). Trade consistency for scale/availability.",
    "Incident post-mortems identify root causes, systemic issues enabling prevention. Blameless culture focuses on systems not individuals. Timeline reconstruction, contributing factors analysis, corrective actions tracking prevent recurrence.",
    "Service mesh (Istio, Linkerd) manages inter-service communication, security policies, traffic management. Sidecars intercept traffic, enforcing policies transparently. Observability integrates tracing, metrics collection.",
    "A/B testing hypotheses validate product decisions through controlled experiments. Treatment vs. control groups compare outcomes; statistical significance confirms findings (p-value <0.05 typical threshold).",
    "Quantum computing challenges: qubit coherence times (microseconds currently), error rates (current 10⁻³), scalability (1M+ qubits needed for practical algorithms). NISQ era (noisy intermediate-scale quantum) focuses on near-term applications.",
    "Software supply chain security: dependency scanning (npm audit, Snyk), manifest verification, code review, artifact signing, SBOM (software bill of materials) enable risk visibility, compromised dependency response.",
    "Genomic variant interpretation: ACMG guidelines classify pathogenicity (benign, likely benign, variant of uncertain significance, likely pathogenic, pathogenic). Population databases (gnomAD, ClinVar) provide evidence. Functional prediction tools assess deleteriousness.",
    "Recommendation system evaluation: precision/recall measure accuracy, coverage measures catalog inclusion, diversity/novelty assess exploration, serendipity captures surprising recommendations.",
    "Secure multi-party computation enables joint computation across parties without revealing inputs. Homomorphic encryption allows computation on encrypted data. Secret sharing distributes secrets among parties preventing individual reconstruction.",
    "Product management balances stakeholder needs: customers (value), business (revenue), technical (feasibility). Discovery phase validates ideas; delivery tracks progress; metrics measure success; iteration improves.",
    "Sentiment analysis classifies text emotional tone (positive, negative, neutral). Feature engineering extracts relevant signals (n-grams, embeddings). Language models (BERT, RoBERTa) capture semantic understanding enabling downstream tasks.",
    "Cache invalidation strategies: TTL (time-to-live), LRU (least recently used), LFU (least frequently used), write-through (update cache+storage), write-behind (cache update, delayed storage). Consistency challenges arise.",
    "Vertical vs. horizontal scaling: vertical (single machine improvements) limited by hardware; horizontal (multiple machines) enables elasticity. Load balancing distributes traffic; statelessness enables horizontal scaling.",
    "Functional programming principles: pure functions (no side effects), immutability (unchanging data), first-class functions (functions as values), higher-order functions. Enables reasoning, parallelization, testability.",
    "SMART goals framework: Specific (well-defined), Measurable (quantifiable), Achievable (realistic), Relevant (business-aligned), Time-bound (deadline). Enables progress tracking, accountability.",
    "Domain-driven design: bounded contexts separate domains, ubiquitous language aligns terminology, aggregate roots enforce consistency, repositories abstract data access. Enables maintainability.",
    "B2B vs. B2C business models: business-to-business targets organizational buyers (longer sales cycles, complex requirements); business-to-consumer targets end-users (faster cycles, simpler needs).",
    "Observability maturity: monitoring (predefined alerts), logging (events), metrics (quantitative), traces (workflows). Cost increases with detail; balance instrumentation cost/benefit.",
    "Canary deployment: route small traffic percentage new version; monitor error rates, latency; promote if healthy, rollback if issues detected. Gradual rollout reduces risk.",
    "API versioning: URL path (/v2/), custom headers, content negotiation (Accept), query parameter (?version=2). Deprecation policies communicate timelines, dual-support eases migration.",
    "Privacy-preserving analytics: differential privacy adds noise obscuring individual records; federated learning trains on-device without centralizing data; synthetic data generation preserves patterns without PII.",
    "Blockchain immutability: hash chain links blocks (previous block hash included); alteration requires recalculating all subsequent hashes detecting tampering; distributed consensus validates blocks preventing unauthorized changes.",
    "Organizational learning: knowledge management captures institutional wisdom; incident learning focuses on systems not individuals; experimentation culture encourages controlled risk-taking.",
    "Vendor lock-in risks: proprietary APIs prevent switching; long-term contracts limit flexibility; data portability concerns. Mitigation: multi-cloud strategies, open standards, contractual escape clauses.",
    "Mathematical notation in technical writing: Greek letters (α, β, γ, μ, σ), mathematical symbols (≈, ≥, →, ∈), set notation (∪, ∩), summation (Σ), probability (P(X), E[X])."
]

tiny_en = {
    "version": 1,
    "id": "standard-tiny-en",
    "targets": ["huggingface", "sentencepiece", "tiktoken"],
    "length": "Tiny",
    "description": "Tiny (~256 tokens)",
    "single": {
        "text": "Programming language paradigms: imperative (how), functional (what), object-oriented (data+behavior), declarative (desired outcome). Multi-paradigm languages (Python, JavaScript) combine approaches enabling flexibility. Language selection balances productivity, performance, ecosystem maturity, team expertise."
    },
    "batch": {
        "count": 100,
        "texts": tiny_en_texts
    }
}

# Write files
import pathlib
base_path = pathlib.Path("/home/user/tests/_templates")

with open(base_path / "tokenization-standard-medium-en.json", "w") as f:
    json.dump(medium_en, f, indent=2)

with open(base_path / "tokenization-standard-short-en.json", "w") as f:
    json.dump(short_en, f, indent=2)

with open(base_path / "tokenization-standard-tiny-en.json", "w") as f:
    json.dump(tiny_en, f, indent=2)

print("Generated medium-en.json, short-en.json, tiny-en.json successfully!")
